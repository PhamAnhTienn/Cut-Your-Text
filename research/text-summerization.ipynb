{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-15T00:04:52.065178Z","iopub.execute_input":"2024-08-15T00:04:52.065971Z","iopub.status.idle":"2024-08-15T00:04:53.109945Z","shell.execute_reply.started":"2024-08-15T00:04:52.065938Z","shell.execute_reply":"2024-08-15T00:04:53.109032Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Thu Aug 15 00:04:52 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   39C    P0             26W /  250W |       0MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q","metadata":{"execution":{"iopub.status.busy":"2024-08-15T00:04:54.259179Z","iopub.execute_input":"2024-08-15T00:04:54.260077Z","iopub.status.idle":"2024-08-15T00:05:13.168153Z","shell.execute_reply.started":"2024-08-15T00:04:54.260039Z","shell.execute_reply":"2024-08-15T00:05:13.166879Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade accelerate\n!pip uninstall -y transformers accelerate\n!pip install transformers accelerate","metadata":{"execution":{"iopub.status.busy":"2024-08-15T00:05:44.499772Z","iopub.execute_input":"2024-08-15T00:05:44.500276Z","iopub.status.idle":"2024-08-15T00:06:23.205290Z","shell.execute_reply.started":"2024-08-15T00:05:44.500238Z","shell.execute_reply":"2024-08-15T00:06:23.204345Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.32.1)\nCollecting accelerate\n  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy<2.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.4)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.5.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.32.1\n    Uninstalling accelerate-0.32.1:\n      Successfully uninstalled accelerate-0.32.1\nSuccessfully installed accelerate-0.33.0\nFound existing installation: transformers 4.42.3\nUninstalling transformers-4.42.3:\n  Successfully uninstalled transformers-4.42.3\nFound existing installation: accelerate 0.33.0\nUninstalling accelerate-0.33.0:\n  Successfully uninstalled accelerate-0.33.0\nCollecting transformers\n  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting accelerate\n  Using cached accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hUsing cached accelerate-0.33.0-py3-none-any.whl (315 kB)\nInstalling collected packages: accelerate, transformers\nSuccessfully installed accelerate-0.33.0 transformers-4.44.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline, set_seed\nfrom datasets import load_dataset, load_from_disk\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset\nimport pandas as pd\nfrom datasets import load_dataset, load_metric\n\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nimport nltk\nfrom nltk.tokenize import sent_tokenize\n\nfrom tqdm import tqdm\nimport torch\n\nnltk.download(\"punkt\")","metadata":{"execution":{"iopub.status.busy":"2024-08-15T00:06:33.674766Z","iopub.execute_input":"2024-08-15T00:06:33.675506Z","iopub.status.idle":"2024-08-15T00:06:52.083087Z","shell.execute_reply.started":"2024-08-15T00:06:33.675471Z","shell.execute_reply":"2024-08-15T00:06:52.082164Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-08-15 00:06:40.204863: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-15 00:06:40.205012: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-15 00:06:40.330214: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-08-15T00:07:24.060108Z","iopub.execute_input":"2024-08-15T00:07:24.061374Z","iopub.status.idle":"2024-08-15T00:07:24.101942Z","shell.execute_reply.started":"2024-08-15T00:07:24.061333Z","shell.execute_reply":"2024-08-15T00:07:24.101036Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"model_ckpt = \"facebook/bart-large-cnn\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\nmodel_bart = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T00:07:28.118239Z","iopub.execute_input":"2024-08-15T00:07:28.118949Z","iopub.status.idle":"2024-08-15T00:07:38.815973Z","shell.execute_reply.started":"2024-08-15T00:07:28.118918Z","shell.execute_reply":"2024-08-15T00:07:38.815186Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76a1dc8f3b384fb4910349591a8cddc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fedf5e82e9c14f419d78ca565beb42b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3783cc6b7084eacacef17e4359efd36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07686a0b93ee4207b4bad53cfe0494c9"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a47930cf99744240a19b41dedacb25ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05295080754644b19e4745911c2c87d3"}},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset_dialogsum = load_dataset(\"knkarthick/dialogsum\")\ndataset_dialogsum","metadata":{"execution":{"iopub.status.busy":"2024-08-15T00:07:51.707155Z","iopub.execute_input":"2024-08-15T00:07:51.707525Z","iopub.status.idle":"2024-08-15T00:07:56.277951Z","shell.execute_reply.started":"2024-08-15T00:07:51.707495Z","shell.execute_reply":"2024-08-15T00:07:56.277076Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/4.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b38a72d4e8b412e838e9efeefb7668a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ae96782b099429296ab6fdbfb652b39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c26fd8209c864868a77ca94ad387225a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63eb8a9aeaae4c63b4f728ddbe71fac6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/12460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23a98c09f1354d49b75926cc2b6ecfb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"758e3e82dbf84c86baae72d10420276f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56aef4907c4c4dcf8469a1c5a5b942fa"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 12460\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 500\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 1500\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"split_lengths = [len(dataset_dialogsum[split])for split in dataset_dialogsum]\n\nprint(f\"Split lengths: {split_lengths}\")\nprint(f\"Features: {dataset_dialogsum['train'].column_names}\")\nprint(\"\\nDialogue:\")\n\nprint(dataset_dialogsum[\"test\"][1][\"dialogue\"])\n\nprint(\"\\nSummary:\")\n\nprint(dataset_dialogsum[\"test\"][1][\"summary\"])","metadata":{"execution":{"iopub.status.busy":"2024-08-15T00:08:06.544813Z","iopub.execute_input":"2024-08-15T00:08:06.545422Z","iopub.status.idle":"2024-08-15T00:08:06.554368Z","shell.execute_reply.started":"2024-08-15T00:08:06.545389Z","shell.execute_reply":"2024-08-15T00:08:06.553378Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Split lengths: [12460, 500, 1500]\nFeatures: ['id', 'dialogue', 'summary', 'topic']\n\nDialogue:\n#Person1#: Ms. Dawson, I need you to take a dictation for me.\n#Person2#: Yes, sir...\n#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n#Person2#: Yes, sir. Go ahead.\n#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n#Person2#: This applies to internal and external communications.\n#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n#Person2#: Is that all?\n#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n\nSummary:\nIn order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\n","output_type":"stream"}]},{"cell_type":"code","source":"def convert_examples_to_features(example_batch):\n    input_encodings = tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )\n\n    with tokenizer.as_target_tokenizer():\n        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n\n    return {\n        'input_ids' : input_encodings['input_ids'],\n        'attention_mask': input_encodings['attention_mask'],\n        'labels': target_encodings['input_ids']\n    }","metadata":{"execution":{"iopub.status.busy":"2024-08-15T00:08:08.949112Z","iopub.execute_input":"2024-08-15T00:08:08.949476Z","iopub.status.idle":"2024-08-15T00:08:08.955134Z","shell.execute_reply.started":"2024-08-15T00:08:08.949446Z","shell.execute_reply":"2024-08-15T00:08:08.954195Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"dataset_dialogsum_pt = dataset_dialogsum.map(convert_examples_to_features, batched = True)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T00:08:11.270135Z","iopub.execute_input":"2024-08-15T00:08:11.271042Z","iopub.status.idle":"2024-08-15T00:08:17.569410Z","shell.execute_reply.started":"2024-08-15T00:08:11.270986Z","shell.execute_reply":"2024-08-15T00:08:17.568500Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b84ea68c96bd49838f861c4cfd186f5e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc977709e14b4d8084511dd62190c7ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e39d8f0d5b949ddb5871b21e65f366e"}},"metadata":{}}]},{"cell_type":"code","source":"dataset_dialogsum_pt[\"train\"]","metadata":{"execution":{"iopub.status.busy":"2024-08-15T00:08:22.218426Z","iopub.execute_input":"2024-08-15T00:08:22.218801Z","iopub.status.idle":"2024-08-15T00:08:22.225205Z","shell.execute_reply.started":"2024-08-15T00:08:22.218770Z","shell.execute_reply":"2024-08-15T00:08:22.224211Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'attention_mask', 'labels'],\n    num_rows: 12460\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Training\nfrom transformers import DataCollatorForSeq2Seq\n\nseq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_bart)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T00:08:25.935497Z","iopub.execute_input":"2024-08-15T00:08:25.935858Z","iopub.status.idle":"2024-08-15T00:08:25.940644Z","shell.execute_reply.started":"2024-08-15T00:08:25.935827Z","shell.execute_reply":"2024-08-15T00:08:25.939504Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntrainer_args = TrainingArguments(\n    output_dir='/kaggle/working/bart-dialogsum',   # Đường dẫn lưu trữ output\n    num_train_epochs=3,                            # Số epoch, 3 là khởi đầu tốt\n    warmup_steps=500,                              # Giảm số warmup steps để tiết kiệm bộ nhớ\n    per_device_train_batch_size=2,                 # Giảm batch size xuống 2 để tránh quá tải bộ nhớ\n    per_device_eval_batch_size=2,                  # Giảm batch size tương tự cho evaluation\n    weight_decay=0.01,                             # Regularization để tránh overfitting\n    logging_steps=50,                              # Tăng logging steps để có nhiều thông tin hơn khi training\n    eval_strategy='steps',                         # Đánh giá mô hình sau mỗi vài steps\n    eval_steps=1000,                               # Đánh giá sau mỗi 1000 steps\n    save_steps=1000,                               # Lưu checkpoint sau mỗi 1000 steps\n    gradient_accumulation_steps=8,                 # Tăng số bước tích lũy gradient để mô phỏng batch size lớn hơn\n    learning_rate=5e-5,                            # Learning rate phổ biến cho BART\n    fp16=True,                                     # Kích hoạt mixed precision để tiết kiệm bộ nhớ\n    save_total_limit=3                             # Giới hạn số lượng checkpoint được lưu\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T00:08:29.843634Z","iopub.execute_input":"2024-08-15T00:08:29.844225Z","iopub.status.idle":"2024-08-15T00:08:30.382557Z","shell.execute_reply.started":"2024-08-15T00:08:29.844194Z","shell.execute_reply":"2024-08-15T00:08:30.381658Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(model=model_bart, args=trainer_args,\n                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n                  train_dataset=dataset_dialogsum_pt[\"train\"],\n                  eval_dataset=dataset_dialogsum_pt[\"validation\"])","metadata":{"execution":{"iopub.status.busy":"2024-08-15T00:08:39.081072Z","iopub.execute_input":"2024-08-15T00:08:39.081875Z","iopub.status.idle":"2024-08-15T00:08:39.719012Z","shell.execute_reply.started":"2024-08-15T00:08:39.081844Z","shell.execute_reply":"2024-08-15T00:08:39.718057Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T00:08:41.966564Z","iopub.execute_input":"2024-08-15T00:08:41.967215Z","iopub.status.idle":"2024-08-15T01:10:17.051162Z","shell.execute_reply.started":"2024-08-15T00:08:41.967182Z","shell.execute_reply":"2024-08-15T01:10:17.050230Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240815_000853-llx5bm81</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/anhtien130805-vietnam-national-university-hanoi/huggingface/runs/llx5bm81' target=\"_blank\">/kaggle/working/bart-dialogsum</a></strong> to <a href='https://wandb.ai/anhtien130805-vietnam-national-university-hanoi/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/anhtien130805-vietnam-national-university-hanoi/huggingface' target=\"_blank\">https://wandb.ai/anhtien130805-vietnam-national-university-hanoi/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/anhtien130805-vietnam-national-university-hanoi/huggingface/runs/llx5bm81' target=\"_blank\">https://wandb.ai/anhtien130805-vietnam-national-university-hanoi/huggingface/runs/llx5bm81</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2334' max='2334' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2334/2334 1:01:04, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1000</td>\n      <td>0.848800</td>\n      <td>1.095185</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.575200</td>\n      <td>1.153857</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2334, training_loss=0.8536778011174925, metrics={'train_runtime': 3694.5824, 'train_samples_per_second': 10.118, 'train_steps_per_second': 0.632, 'total_flos': 2.098023063768269e+16, 'train_loss': 0.8536778011174925, 'epoch': 2.997110754414125})"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import GenerationConfig\n\n# Evaluation\n\ndef generate_batch_sized_chunks(list_of_elements, batch_size):\n    \"\"\"split the dataset into smaller batches that we can process simultaneously\n    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n    for i in range(0, len(list_of_elements), batch_size):\n        yield list_of_elements[i : i + batch_size]\n\ndef calculate_metric_on_test_ds(dataset, metric, model, tokenizer, \n                               batch_size=16, device=device, \n                               column_text=\"article\", \n                               column_summary=\"highlights\"):\n    \n    # Tạo đối tượng GenerationConfig\n    generation_config = GenerationConfig(\n        max_length=150,\n        min_length=30,\n        early_stopping=True,\n        num_beams=4,\n        length_penalty=1.0,\n        no_repeat_ngram_size=3,\n    )\n    \n    # Tạo các batch từ dữ liệu\n    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n\n    for article_batch, target_batch in tqdm(\n        zip(article_batches, target_batches), total=len(article_batches)):\n        \n        inputs = tokenizer(article_batch, max_length=1024,  truncation=True, \n                        padding=\"max_length\", return_tensors=\"pt\")\n        \n        # Sử dụng GenerationConfig khi sinh văn bản\n        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n                                   attention_mask=inputs[\"attention_mask\"].to(device), \n                                   generation_config=generation_config)\n        \n        # Giải mã và xử lý hậu kỳ\n        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, \n                             clean_up_tokenization_spaces=True) \n                             for s in summaries]      \n        \n        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n        \n        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n        \n    # Tính toán và trả về các chỉ số ROUGE\n    score = metric.compute()\n    return score","metadata":{"execution":{"iopub.status.busy":"2024-08-15T01:15:10.760038Z","iopub.execute_input":"2024-08-15T01:15:10.760815Z","iopub.status.idle":"2024-08-15T01:15:10.777871Z","shell.execute_reply.started":"2024-08-15T01:15:10.760776Z","shell.execute_reply":"2024-08-15T01:15:10.777056Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-08-15T01:17:42.436532Z","iopub.execute_input":"2024-08-15T01:17:42.437296Z","iopub.status.idle":"2024-08-15T01:17:55.359307Z","shell.execute_reply.started":"2024-08-15T01:17:42.437263Z","shell.execute_reply":"2024-08-15T01:17:55.358212Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.20.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.5.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.23.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from evaluate import load\n\nrouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\nrouge_metric = load(\"rouge\")","metadata":{"execution":{"iopub.status.busy":"2024-08-15T01:18:00.579158Z","iopub.execute_input":"2024-08-15T01:18:00.580015Z","iopub.status.idle":"2024-08-15T01:18:01.403275Z","shell.execute_reply.started":"2024-08-15T01:18:00.579965Z","shell.execute_reply":"2024-08-15T01:18:01.402069Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd8fee415d0b457ea64042566372268a"}},"metadata":{}}]},{"cell_type":"code","source":"# Tính toán score\nscore = calculate_metric_on_test_ds(\n    dataset_dialogsum['test'][0:10], rouge_metric, trainer.model, tokenizer, \n    batch_size=2, column_text='dialogue', column_summary='summary'\n)\n\n# Kiểm tra cấu trúc của score để xác định cách truy cập đúng\nprint(score)\n\n# Giả sử score[rn] đã là giá trị fmeasure trực tiếp\nrouge_dict = dict((rn, score[rn]) for rn in rouge_names)\n\n# Tạo DataFrame từ kết quả\nimport pandas as pd\npd.DataFrame(rouge_dict, index=[f'bart'])","metadata":{"execution":{"iopub.status.busy":"2024-08-15T01:19:30.132447Z","iopub.execute_input":"2024-08-15T01:19:30.132822Z","iopub.status.idle":"2024-08-15T01:19:38.316270Z","shell.execute_reply.started":"2024-08-15T01:19:30.132792Z","shell.execute_reply":"2024-08-15T01:19:38.315341Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"100%|██████████| 5/5 [00:07<00:00,  1.58s/it]\n","output_type":"stream"},{"name":"stdout","text":"{'rouge1': 0.0043410677187272935, 'rouge2': 0.0, 'rougeL': 0.004376886903482648, 'rougeLsum': 0.00427556978088893}\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"        rouge1  rouge2    rougeL  rougeLsum\nbart  0.004341     0.0  0.004377   0.004276","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rougeL</th>\n      <th>rougeLsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>bart</th>\n      <td>0.004341</td>\n      <td>0.0</td>\n      <td>0.004377</td>\n      <td>0.004276</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"model_bart.save_pretrained(\"./bart-dialogsum-model\")","metadata":{"execution":{"iopub.status.busy":"2024-08-15T01:25:21.739133Z","iopub.execute_input":"2024-08-15T01:25:21.739521Z","iopub.status.idle":"2024-08-15T01:25:25.701424Z","shell.execute_reply.started":"2024-08-15T01:25:21.739491Z","shell.execute_reply":"2024-08-15T01:25:25.700411Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"}]},{"cell_type":"code","source":"## Save tokenizer\ntokenizer.save_pretrained(\"./tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2024-08-15T01:25:30.392069Z","iopub.execute_input":"2024-08-15T01:25:30.392449Z","iopub.status.idle":"2024-08-15T01:25:30.452755Z","shell.execute_reply.started":"2024-08-15T01:25:30.392420Z","shell.execute_reply":"2024-08-15T01:25:30.451715Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"('./tokenizer/tokenizer_config.json',\n './tokenizer/special_tokens_map.json',\n './tokenizer/vocab.json',\n './tokenizer/merges.txt',\n './tokenizer/added_tokens.json',\n './tokenizer/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"#Load\ntokenizer = AutoTokenizer.from_pretrained(\"./tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2024-08-15T01:25:33.369718Z","iopub.execute_input":"2024-08-15T01:25:33.370118Z","iopub.status.idle":"2024-08-15T01:25:33.439748Z","shell.execute_reply.started":"2024-08-15T01:25:33.370087Z","shell.execute_reply":"2024-08-15T01:25:33.438593Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Prediction\ngen_kwargs = {\n    \"length_penalty\": 1.0, \n    \"num_beams\": 4, \n    \"max_length\": 150, \n    \"min_length\": 30, \n    \"early_stopping\": True\n}\n\nsample_text = dataset_dialogsum[\"test\"][0][\"dialogue\"]\n\nreference = dataset_dialogsum[\"test\"][0][\"summary\"]\n\npipe = pipeline(\"summarization\", model=\"bart-dialogsum-model\", tokenizer=tokenizer)\n\nprint(\"Dialogue:\")\nprint(sample_text)\n\nprint(\"\\nReference Summary:\")\nprint(reference)\n\nprint(\"\\nModel Summary:\")\nprint(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])","metadata":{"execution":{"iopub.status.busy":"2024-08-15T01:25:48.349524Z","iopub.execute_input":"2024-08-15T01:25:48.349906Z","iopub.status.idle":"2024-08-15T01:25:59.249976Z","shell.execute_reply.started":"2024-08-15T01:25:48.349875Z","shell.execute_reply":"2024-08-15T01:25:59.249053Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"},{"name":"stdout","text":"Dialogue:\n#Person1#: Ms. Dawson, I need you to take a dictation for me.\n#Person2#: Yes, sir...\n#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n#Person2#: Yes, sir. Go ahead.\n#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n#Person2#: This applies to internal and external communications.\n#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n#Person2#: Is that all?\n#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n\nReference Summary:\nMs. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n\nModel Summary:\n#Person1# asks Ms. Dawson to take a dictation and tells her that all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}